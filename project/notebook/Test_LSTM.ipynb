{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe6H8hB0z7QN",
    "outputId": "060c3d71-ebe5-4428-bd01-b936b130faf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=xVSRbIuA8DAL7xGySEWbd3G7cNlOEv&access_type=offline&code_challenge=uxjXyGsWFO5HMPqQp6Zj04VnzvouWmSnVzD6jtBXzDE&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Application Default Credentials (ADC) were updated.\n",
      "\n",
      "You are now logged in as [biyan.bahtiar.1@gmail.com].\n",
      "Your current project is [b-508911].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login --update-adc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b-508911\n"
     ]
    }
   ],
   "source": [
    "!gcloud config get-value project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wHl2itlK6ZqY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump,load\n",
    "from google.cloud import storage, bigquery\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BpICQcNb7Xnw"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZNYL6oz8E_7",
    "outputId": "810a241b-3486-4642-ad6e-6449ce24a3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ml_ops': ['bank_campaign_model_metrics'], 'test_dataset_iykra': ['cost_test', 'test_table', 'view1'], 'test_schema': ['us_states']}\n"
     ]
    }
   ],
   "source": [
    "datasets = [obj.dataset_id for obj in list(bq_client.list_datasets())]\n",
    "tables = {dataset: [table.table_id for table in bq_client.list_tables(dataset)] for dataset in datasets}\n",
    "\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "S3cCIgsfAZH6",
    "outputId": "7e7881e9-33a7-4fd8-f620-cdd5a7ed3f03"
   },
   "outputs": [],
   "source": [
    "project_id = 'b-508911'\n",
    "dataset_id = 'test_dataset_iykra'\n",
    "table_id = 'test_table'\n",
    "\n",
    "query_all_test_table = f\"\"\"\n",
    "select *\n",
    "from {project_id}.{dataset_id}.{table_id}\n",
    "where Date >= date_sub(current_date(), interval 1 year)\n",
    "\"\"\"\n",
    "\n",
    "test_table_df = bq_client.query(query_all_test_table).to_dataframe()\n",
    "test_table_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jM9LiEcxA-qb",
    "outputId": "8dcab03f-63fc-41c6-dbb0-49fe9791e2d7"
   },
   "outputs": [],
   "source": [
    "test_table_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pReOJZ7eEJTy"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQw4Miz0JsIr",
    "outputId": "0d0084de-1e05-436c-ca86-a8af6e21dfbf"
   },
   "outputs": [],
   "source": [
    "min_date = test_table['Date'].min()\n",
    "max_date = test_table['Date'].max()\n",
    "\n",
    "print(f'min_date:{min_date}, max_date:{max_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gDBA6_zIBJj"
   },
   "outputs": [],
   "source": [
    "cols, names = list(), list()\n",
    "\n",
    "df = test_table_df[['Date', 'Warehouse_ID', 'Product_ID', 'Discount_Impact', 'Sales_Event', 'Daily_Sales']]\n",
    "df = df.sort_values(by=['Warehouse_ID', 'Product_ID', 'Date'])\n",
    "\n",
    "tes = pd.DataFrame()\n",
    "\n",
    "for i in range(30, 0, -1):\n",
    "    cols.append(df.shift(i))\n",
    "    names += [('%s(t-%d)' % (col, i)) for col in df.columns]\n",
    "    tes = pd.concat(cols, axis=1)\n",
    "    tes.columns = names\n",
    "\n",
    "display(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GDbRwROH8dr"
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
    "    cols, names = list(), list()\n",
    "    # Input sequence (t-n, ... t-1)\n",
    "    for i in range(window, 0, -1):\n",
    "        cols.append(data.shift(i))\n",
    "        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "    # Current timestep (t=0)\n",
    "    cols.append(data)\n",
    "    names += [('%s(t)' % (col)) for col in data.columns]\n",
    "    # Target timestep (t=lag)\n",
    "    cols.append(data.shift(-lag))\n",
    "    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # Drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7iG6wXuEHUJ"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def preprocess_data(data):\n",
    "\n",
    "    # Filter columns\n",
    "    data = data[['Date', 'Warehouse_ID', 'Product_ID', 'Promotion', 'Discount', 'Sales']]\n",
    "\n",
    "    # Sort data\n",
    "    data = data.sort_values(by=['Warehouse_ID', 'Product_ID', 'Date'])\n",
    "\n",
    "    # Filter None results and combine\n",
    "    results = [r for r in results if r is not None]\n",
    "\n",
    "    if not results:\n",
    "        raise ValueError(\"No valid sequences generated!\")\n",
    "\n",
    "    sequences = np.concatenate([r[0] for r in results])\n",
    "    targets = np.concatenate([r[1] for r in results])\n",
    "\n",
    "    print(f\"Generated {len(sequences)} sequences\")\n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6ouxs-EzJ8M"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom dataset class with memory optimization\n",
    "class SalesDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# LSTM Model with performance optimizations\n",
    "class SalesLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(SalesLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Added batch normalization for better training stability\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Optimized preprocessing for a single store-item group\n",
    "def process_group(group_data):\n",
    "    try:\n",
    "        store, item = group_data[0]\n",
    "        group = group_data[1]\n",
    "\n",
    "        if len(group) < 120:  # Minimum required length\n",
    "            return None\n",
    "\n",
    "        # Normalize numerical features\n",
    "        scaler = StandardScaler()\n",
    "        group['sales'] = scaler.fit_transform(group[['sales']])\n",
    "        group['discount'] = scaler.fit_transform(group[['discount']])\n",
    "\n",
    "        sequences = []\n",
    "        targets = []\n",
    "\n",
    "        # Create sequences with larger step size\n",
    "        step_size = 3  # Create sequence every 3 days\n",
    "        for i in range(0, len(group) - 119, step_size):  # 119 = window_size + horizon - 1\n",
    "            sequence = group.iloc[i:i+30]  # window_size = 30\n",
    "            target = group.iloc[i+30:i+120]['sales'].values  # 90 days prediction\n",
    "\n",
    "            feature_vector = np.column_stack((\n",
    "                sequence['sales'].values,\n",
    "                sequence['promotion'].values,\n",
    "                sequence['discount'].values,\n",
    "                sequence['store'].values,\n",
    "                sequence['item'].values\n",
    "            ))\n",
    "\n",
    "            sequences.append(feature_vector)\n",
    "            targets.append(target)\n",
    "\n",
    "        return np.array(sequences), np.array(targets)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing store {store}, item {item}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Optimized training function\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs, device):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Use tqdm for progress bar\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch_features, batch_targets in train_pbar:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            # Mixed precision training\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        model.eval()\n",
    "        total_valid_loss = 0\n",
    "\n",
    "        valid_pbar = tqdm(valid_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Valid]')\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in valid_pbar:\n",
    "                batch_features = batch_features.to(device)\n",
    "                batch_targets = batch_targets.to(device)\n",
    "\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                total_valid_loss += loss.item()\n",
    "                valid_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]: '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, '\n",
    "              f'Valid Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    HIDDEN_SIZE = 128\n",
    "    NUM_LAYERS = 2\n",
    "    BATCH_SIZE = 256  # Increased batch size\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    N_SAMPLES = 1000  # Number of store-item combinations to use\n",
    "\n",
    "    # Preprocess data\n",
    "    sequences, targets = preprocess_data(df, n_samples=N_SAMPLES)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        sequences, targets, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = SalesDataset(X_train, y_train)\n",
    "    valid_dataset = SalesDataset(X_valid, y_valid)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,  # Parallel data loading\n",
    "        pin_memory=True  # Faster data transfer to GPU\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    input_size = sequences.shape[2]\n",
    "    output_size = targets.shape[1]\n",
    "\n",
    "    model = SalesLSTM(\n",
    "        input_size=input_size,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        output_size=output_size\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Add learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_losses, valid_losses = train_model(\n",
    "        model, train_loader, valid_loader,\n",
    "        criterion, optimizer, NUM_EPOCHS, device\n",
    "    )\n",
    "\n",
    "    return model, train_losses, valid_losses\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, train_losses, valid_losses = main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
